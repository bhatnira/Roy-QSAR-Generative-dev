"""
Validation Runner Module
=========================

Orchestrates comprehensive QSAR validation workflow.
"""

import pandas as pd
from typing import Dict

from .dataset_analysis import DatasetBiasAnalyzer
from .activity_cliffs import ActivityCliffDetector
from .assay_noise import AssayNoiseEstimator


def run_comprehensive_validation(df: pd.DataFrame, 
                                 smiles_col: str = 'Canonical SMILES',
                                 target_col: str = 'IC50 uM') -> Dict:
    """
    Run all validation checks in sequence.
    
    Args:
        df: DataFrame with SMILES and target columns
        smiles_col: Name of SMILES column (default: 'Canonical SMILES')
        target_col: Name of target column (default: 'IC50 uM')
        
    Returns:
        Dictionary with all validation results
    """
    results = {}
    
    # 1. Dataset bias analysis
    bias_analyzer = DatasetBiasAnalyzer(smiles_col, target_col)
    results['scaffold_diversity'] = bias_analyzer.analyze_scaffold_diversity(df)
    results['activity_distribution'] = bias_analyzer.analyze_activity_distribution(df)
    
    # 2. Activity cliffs
    cliff_detector = ActivityCliffDetector(smiles_col, target_col)
    results['activity_cliffs'] = cliff_detector.detect_activity_cliffs(df)
    
    # 3. Assay noise
    noise_estimator = AssayNoiseEstimator()
    results['experimental_error'] = noise_estimator.estimate_experimental_error(df, target_col)
    
    print("\n" + "=" * 70)
    print("âœ“ Comprehensive validation complete")
    print("=" * 70)
    
    return results


def print_comprehensive_validation_checklist():
    """
    Print comprehensive checklist for QSAR model validation.
    """
    checklist = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     COMPREHENSIVE QSAR VALIDATION CHECKLIST (Low-Data Regime)        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ”´ CRITICAL (Must Fix)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â˜ 1. Data Leakage Prevention
       â€¢ Scaffold-based splitting (not random)
       â€¢ Duplicates removed BEFORE splitting
       â€¢ Features scaled using training data only
       â€¢ No feature selection on full dataset
    
    â˜ 2. Dataset Bias & Representativeness
       â€¢ Scaffold diversity analyzed
       â€¢ Scaffold counts reported per split
       â€¢ Chemical space limitations acknowledged
       â€¢ Congeneric series identified
    
    â˜ 3. Model Complexity Control
       â€¢ Samples-to-features ratio â‰¥ 5 (preferably â‰¥ 10)
       â€¢ Regularization applied appropriately
       â€¢ Hyperparameter ranges restricted
       â€¢ Nested CV for hyperparameter tuning
    
    â˜ 4. Proper Cross-Validation
       â€¢ Scaffold-based CV (not random)
       â€¢ Report mean Â± std across folds
       â€¢ No feature selection in CV loop
       â€¢ Proper pipeline in each fold
    
    ğŸŸ  HIGH PRIORITY (Strongly Recommended)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â˜ 5. Assay Noise Consideration
       â€¢ Experimental error estimated/reported
       â€¢ RMSE compared to assay precision
       â€¢ Mixed assay types identified
       â€¢ Suspicious RMSE < 0.3 log units flagged
    
    â˜ 6. Activity Cliffs
       â€¢ Activity cliffs detected and reported
       â€¢ SAR discontinuities acknowledged
       â€¢ Local models considered if many cliffs
       â€¢ Feature importance interpreted cautiously
    
    â˜ 7. Proper Metrics & Baselines
       â€¢ RMSE, MAE, RÂ², Spearman Ï all reported
       â€¢ Baseline model (Ridge) compared
       â€¢ External test set evaluated
       â€¢ Not just RÂ² (can be misleading)
    
    â˜ 8. Y-Randomization Test
       â€¢ Y-scrambling performed (10+ iterations)
       â€¢ RÂ² should be â‰¤ 0 with random targets
       â€¢ Results reported in supplementary
    
    ğŸŸ¡ MODERATE PRIORITY (Best Practices)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â˜ 9. Target Distribution Analysis
       â€¢ Activity range reported
       â€¢ Distribution visualized
       â€¢ Narrow ranges acknowledged
       â€¢ Outliers investigated
    
    â˜ 10. Uncertainty Estimation
       â€¢ Prediction intervals provided (GPR/ensemble)
       â€¢ Applicability domain defined
       â€¢ Out-of-domain predictions flagged
    
    â˜ 11. Interpretability
       â€¢ Mechanistic claims avoided
       â€¢ SHAP/feature importance = hypothesis generation only
       â€¢ Correlation vs causation clear
       â€¢ Validated against known SAR
    
    â˜ 12. Reproducibility
       â€¢ All random seeds fixed
       â€¢ Code/data publicly available
       â€¢ Preprocessing steps documented
       â€¢ Software versions recorded
    
    â˜ 13. Honest Reporting
       â€¢ Applicability domain stated clearly
       â€¢ Limitations acknowledged
       â€¢ Performance drop with scaffold split noted
       â€¢ No cherry-picking of metrics
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ“Š EXPECTED PERFORMANCE CHANGES
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    When fixing data leakage issues, expect:
    
    â€¢ RÂ² drop: 0.80 â†’ 0.60 (or lower)  âœ“ This is NORMAL and CORRECT
    â€¢ RMSE increase: 0.3 â†’ 0.5        âœ“ More realistic
    â€¢ Scaffold split harder than random âœ“ Tests generalization
    
    If performance stays very high after fixes â†’ still may be issues
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ“Œ KEY TAKEAWAYS FOR LOW-DATA QSAR
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. Simpler models often outperform complex ones (n < 200)
    2. Scaffold split is mandatory for honest evaluation
    3. RMSE â‰ˆ 0.5 log units is near theoretical limit for ICâ‚…â‚€
    4. RÂ² alone is misleading with narrow activity ranges
    5. Y-randomization test catches overfitting
    6. Activity cliffs limit local predictivity
    7. Report limitations honestly â†’ better reviews
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(checklist)


if __name__ == "__main__":
    print_comprehensive_validation_checklist()
