# Model 1 - Data Leakage Fixes Applied

## Summary of Changes

**Date:** January 6, 2026  
**Notebook:** Model_1_circular_fingerprint_features_1024_H20_autoML_Model_Interpretation (1).ipynb

---

## ‚úÖ Changes Made

### 1. **Added Data Leakage Prevention Section** (Beginning of notebook)
- New markdown cell explaining the data leakage fixes
- Import cell for `qsar_utils_no_leakage` module
- Displays summary of prevention measures

### 2. **Updated Duplicate Removal** (Cell after data loading)
- **OLD:** Simple `drop_duplicates()` after canonicalization
- **NEW:** Using `QSARDataProcessor` class
  - Proper canonicalization
  - Strategy-based duplicate removal (averaging replicates)
  - Done BEFORE splitting (prevents leakage)

### 3. **Added Scaffold-Based Splitting Section** (After data cleaning)
New section includes:

#### a. Scaffold Split Implementation
```python
splitter = ScaffoldSplitter(smiles_col='cleanedMol')
train_idx, val_idx, test_idx = splitter.scaffold_split(
    numeric_df, test_size=0.2, val_size=0.1, random_state=42
)
```

#### b. Near-Duplicate Removal
```python
train_idx, test_idx = processor.remove_near_duplicates(
    numeric_df, train_idx, test_idx, threshold=0.95
)
```

#### c. Similarity Analysis
```python
similarity_stats = processor.analyze_similarity(numeric_df, train_idx, test_idx)
plot_similarity_distribution(similarity_stats)
```

#### d. Verification Checks
- Checks for SMILES overlap (should be 0)
- Verifies scaffold-based split integrity
- Reports similarity statistics

#### e. Data Preparation
- Creates `train_df_clean`, `val_df_clean`, `test_df_clean`
- Updates `df2` to be training set only
- Stores indices for reproducibility

---

## üö® Critical Changes for Existing Code

### What Changed:
1. **`df2` is now the TRAINING SET ONLY** (not the full dataset)
2. Validation set available as `df_validation`
3. Test set available as `df_test`
4. Original dataset saved as `numeric_df_original`

### What You Need to Update:

#### ‚ùå OLD Approach (in later cells):
```python
# Using entire dataset for feature generation
# Then splitting later...
```

#### ‚úÖ NEW Approach (required):
```python
# Generate features PER SPLIT
X_train = generate_features(df2)  # Training only
X_val = generate_features(df_validation)  # Validation only
X_test = generate_features(df_test)  # Test only

# Scale features (FIT on train ONLY)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # ‚úÖ FIT
X_val_scaled = scaler.transform(X_val)  # ‚úÖ TRANSFORM only
X_test_scaled = scaler.transform(X_test)  # ‚úÖ TRANSFORM only
```

---

## üìã Next Steps

### Immediate Actions Required:

1. **Update Feature Generation Section**
   - Find cells that generate circular fingerprints
   - Generate separately for train/val/test
   - Ensure no information leakage

2. **Update H2O AutoML Section**
   - Convert splits separately to H2OFrame
   - Train on training set only
   - Validate on validation set
   - Final evaluation on test set

3. **Update Cross-Validation**
   - Find cells using `KFold`
   - Replace with scaffold-based CV:
   ```python
   cv_splits = splitter.scaffold_kfold(df2, n_splits=5, random_state=42)
   ```

4. **Update Any Scaling/Normalization**
   - Ensure all scalers are fitted on training data only
   - Transform (not fit_transform) validation and test

### Cells That Still Need Updates:

Look for these patterns in later cells and update them:

1. **Fingerprint Generation**
   - Currently: Generates for entire `numeric_df`
   - Should be: Generate separately for train/val/test

2. **Feature Scaling**
   - Currently: May scale entire dataset
   - Should be: Fit on train, transform val/test

3. **Cross-Validation in H2O**
   - Currently: May use random CV
   - Should be: Use scaffold-based CV splits

4. **Model Evaluation**
   - Currently: May evaluate on random test set
   - Should be: Evaluate on scaffold-split test set

---

## üîç Verification Checklist

After running the updated notebook, verify:

- [ ] Cell with "ALL CHECKS PASSED" displays
- [ ] Train-Test SMILES overlap = 0
- [ ] Train-Val SMILES overlap = 0  
- [ ] Val-Test SMILES overlap = 0
- [ ] Similarity distribution plot generated
- [ ] Mean train-test similarity reported
- [ ] Scaffold overlap = 0% confirmed

---

## üìä Expected Performance Changes

### What to Expect:

| Metric | Before (with leakage) | After (no leakage) | Change |
|--------|----------------------|-------------------|--------|
| Train R¬≤ | 0.90-0.95 | 0.80-0.90 | -5 to -10% |
| **Test R¬≤** | **0.85-0.90** | **0.60-0.75** | **-15 to -30%** |
| Test RMSE | 0.3-0.4 | 0.5-0.7 | +40-75% |

### Why This is Good:
- Performance drop is **expected and correct**
- Old results were **artificially inflated** due to leakage
- New results are **realistic** for unseen scaffolds
- Model is now **scientifically valid**

---

## üìù For Your Paper/Report

### What to Include:

1. **Methods Section:**
```
Data Splitting: We employed scaffold-based splitting using Bemis-Murcko 
scaffolds to prevent data leakage. Entire molecular scaffolds were assigned 
exclusively to training (70%), validation (10%), or test (20%) sets, ensuring 
zero scaffold overlap between splits. Near-duplicate molecules (Tanimoto 
similarity ‚â• 0.95) were removed from the test set to prevent information 
leakage.

Cross-Validation: Five-fold scaffold-based cross-validation was used for 
hyperparameter tuning, ensuring no scaffold overlap between folds.

Feature Scaling: StandardScaler was fitted on training data only and then 
applied to validation and test sets to prevent data leakage.
```

2. **Results Section:**
```
Train-Test Similarity: The mean maximum Tanimoto similarity between test 
molecules and the training set was X.XX ¬± X.XX (median: X.XX, range: 
[X.XX, X.XX]), indicating the test set represents a moderate extrapolation 
challenge.

Model Performance: The model achieved R¬≤ = X.XX on the scaffold-split test 
set, demonstrating reasonable predictive performance on unseen molecular 
scaffolds.
```

3. **Supplementary Materials:**
- Include train-test similarity distribution plot
- Report scaffold overlap statistics (should be 0%)
- List number of near-duplicates removed

---

## üÜò Troubleshooting

### Issue: "Import error for qsar_utils_no_leakage"
**Solution:** Make sure `qsar_utils_no_leakage.py` is in the same directory

### Issue: "Performance dropped significantly"
**Solution:** This is expected! See "Expected Performance Changes" above

### Issue: "Not enough data in test set"
**Solution:** 
- Adjust test_size to 0.15 if needed
- Use repeated scaffold-based CV instead
- Report the limitation in your paper

### Issue: "H2O complains about data format"
**Solution:**
- Convert each split separately to H2OFrame
- Don't pass the entire dataset to H2O
- Use separate frames for train/val/test

---

## üìû Next Model Updates

After confirming Model 1 works:

1. Apply same changes to Model 2 (ChEBERTa)
2. Apply same changes to Model 3 (RDKit features)
3. Apply same changes to Model 4 (Gaussian Process)

Use the same train/val/test splits across all models for consistency!

---

## ‚úÖ Success Criteria

You'll know the fixes are working when:

1. ‚úÖ Verification cell shows "ALL CHECKS PASSED"
2. ‚úÖ No SMILES overlap between any splits
3. ‚úÖ Similarity analysis completes successfully
4. ‚úÖ Model trains on training set only
5. ‚úÖ Performance is more realistic (likely lower but valid)
6. ‚úÖ Results are reproducible

---

**Status:** ‚úÖ Data leakage prevention infrastructure added  
**Next:** Update feature generation and model training sections
