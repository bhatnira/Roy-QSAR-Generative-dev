{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb31c941",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Data Leakage Prevention - Complete Implementation Guide\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates comprehensive data leakage prevention for QSAR models.\n",
    "\n",
    "### Critical Issues Addressed:\n",
    "1. ‚úÖ **Scaffold-based splitting** (not random)\n",
    "2. ‚úÖ **Duplicate & near-duplicate removal**\n",
    "3. ‚úÖ **Feature scaling on train only**\n",
    "4. ‚úÖ **Proper cross-validation**\n",
    "5. ‚úÖ **Target leakage prevention**\n",
    "6. ‚úÖ **Similarity analysis**\n",
    "7. ‚úÖ **Applicability domain**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858bb358",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d19759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data leakage prevention utilities from the framework\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Auto-detect framework path (works when cloned from GitHub)\n",
    "# This finds the 'src' directory relative to the notebook location\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__')) if '__file__' in dir() else os.getcwd()\n",
    "repo_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "framework_path = os.path.join(repo_root, 'src')\n",
    "\n",
    "# Add framework to path if it exists\n",
    "if os.path.exists(framework_path) and framework_path not in sys.path:\n",
    "    sys.path.insert(0, framework_path)\n",
    "    print(f\"‚úì Framework loaded from: {framework_path}\")\n",
    "else:\n",
    "    print(f\"‚ö† Warning: Framework path not found at {framework_path}\")\n",
    "    print(\"  Make sure you're running from the notebooks/ folder in the cloned repo\")\n",
    "\n",
    "# Import core utilities for data processing\n",
    "from utils.qsar_utils_no_leakage import QSARDataProcessor\n",
    "\n",
    "# Import validation modules\n",
    "from qsar_validation.splitting_strategies import AdvancedSplitter\n",
    "from qsar_validation.feature_scaling import FeatureScaler\n",
    "from qsar_validation.feature_selection import FeatureSelector\n",
    "from qsar_validation.dataset_quality_analysis import DatasetQualityAnalyzer\n",
    "from qsar_validation.performance_validation import PerformanceValidator\n",
    "from qsar_validation.activity_cliffs_detection import ActivityCliffsDetector\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(\"‚úÖ Framework v4.1.0 - Multi-Library Support\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QSAR VALIDATION FRAMEWORK - MODULES LOADED:\")\n",
    "print(\"  ‚úì QSARDataProcessor - Duplicate removal & data processing\")\n",
    "print(\"  ‚úì AdvancedSplitter - Scaffold-based splitting\")\n",
    "print(\"  ‚úì FeatureScaler - Proper feature scaling (fit on train only)\")\n",
    "print(\"  ‚úì FeatureSelector - Feature selection to prevent overfitting\")\n",
    "print(\"  ‚úì DatasetQualityAnalyzer - Dataset representativeness checks\")\n",
    "print(\"  ‚úì PerformanceValidator - Cross-validation & metrics\")\n",
    "print(\"  ‚úì ActivityCliffsDetector - Activity cliff analysis\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Replace this path with your actual data path\n",
    "df = pd.read_excel('/content/drive/MyDrive/DrRoyRationalDesign/Input of triazole and cysteine_datasheet.xlsx')\n",
    "\n",
    "print(f\"Original dataset: {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b68c7",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning - Prevent Leakage from Duplicates\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistake:\n",
    "- Random split with duplicates ‚Üí same molecule in train AND test\n",
    "\n",
    "### ‚úÖ Solution:\n",
    "- Canonicalize SMILES\n",
    "- Remove exact duplicates BEFORE splitting\n",
    "- Average replicate measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10670752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = QSARDataProcessor(\n",
    "    smiles_col='Canonical SMILES',\n",
    "    target_col='IC50 uM'\n",
    ")\n",
    "\n",
    "# Step 1: Canonicalize SMILES\n",
    "df = processor.canonicalize_smiles(df)\n",
    "\n",
    "# Step 2: Remove exact duplicates (average replicates)\n",
    "df = processor.remove_duplicates(df, strategy='average')\n",
    "\n",
    "# Step 3: Keep only numeric IC50 values\n",
    "mask = pd.to_numeric(df[\"IC50 uM\"], errors=\"coerce\").notna()\n",
    "df = df[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úì Clean dataset: {len(df)} unique molecules with valid IC50 values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676ffa7",
   "metadata": {},
   "source": [
    "## Step 3: Scaffold-Based Splitting - Critical for QSAR!\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistake:\n",
    "```python\n",
    "# DON'T DO THIS - Random split causes leakage!\n",
    "train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### ‚úÖ Solution: Scaffold-based split\n",
    "- Use Bemis-Murcko scaffolds\n",
    "- Entire scaffold in train OR test (never both)\n",
    "- More realistic performance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaffold splitter from framework\n",
    "splitter = AdvancedSplitter()\n",
    "\n",
    "# Perform scaffold-based split\n",
    "splits = splitter.scaffold_split(\n",
    "    df,\n",
    "    smiles_col='Canonical SMILES',\n",
    "    target_col='IC50 uM',\n",
    "    test_size=0.2,\n",
    "    val_size=0.1\n",
    ")\n",
    "\n",
    "train_idx = splits['train_idx']\n",
    "val_idx = splits['val_idx']\n",
    "test_idx = splits['test_idx']\n",
    "\n",
    "print(f\"\\n‚úì Scaffold-based split completed:\")\n",
    "print(f\"   Training: {len(train_idx)} molecules\")\n",
    "print(f\"   Validation: {len(val_idx)} molecules\")\n",
    "print(f\"   Test: {len(test_idx)} molecules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96814aa",
   "metadata": {},
   "source": [
    "## Step 4: Remove Near-Duplicates Between Splits\n",
    "\n",
    "### ‚úÖ Check for molecules with Tanimoto similarity ‚â• 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove near-duplicates (Tanimoto >= 0.95)\n",
    "train_idx, test_idx = processor.remove_near_duplicates(\n",
    "    df,\n",
    "    train_idx,\n",
    "    test_idx,\n",
    "    threshold=0.95\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì After near-duplicate removal:\")\n",
    "print(f\"   Training: {len(train_idx)} molecules\")\n",
    "print(f\"   Test: {len(test_idx)} molecules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640075c9",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Train-Test Similarity\n",
    "\n",
    "### üìä Important for reviewers:\n",
    "- Shows applicability domain\n",
    "- Demonstrates no data leakage\n",
    "- Realistic performance expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15999aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze similarity between train and test\n",
    "similarity_stats = processor.analyze_similarity(df, train_idx, test_idx)\n",
    "\n",
    "# Plot similarity distribution\n",
    "plot_similarity_distribution(similarity_stats, save_path='similarity_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a8184",
   "metadata": {},
   "source": [
    "## Step 6: Create Splits and Check for Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b58f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test dataframes\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "# CRITICAL CHECK: Ensure no SMILES overlap\n",
    "train_smiles = set(train_df['Canonical SMILES'])\n",
    "val_smiles = set(val_df['Canonical SMILES'])\n",
    "test_smiles = set(test_df['Canonical SMILES'])\n",
    "\n",
    "overlap_train_test = train_smiles & test_smiles\n",
    "overlap_train_val = train_smiles & val_smiles\n",
    "overlap_val_test = val_smiles & test_smiles\n",
    "\n",
    "print(\"\\nüîç Checking for SMILES overlap (should all be 0):\")\n",
    "print(f\"   Train-Test overlap: {len(overlap_train_test)} molecules\")\n",
    "print(f\"   Train-Val overlap: {len(overlap_train_val)} molecules\")\n",
    "print(f\"   Val-Test overlap: {len(overlap_val_test)} molecules\")\n",
    "\n",
    "if overlap_train_test or overlap_train_val or overlap_val_test:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: DATA LEAKAGE DETECTED!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No data leakage detected - splits are clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c901103b",
   "metadata": {},
   "source": [
    "## Step 7: Feature Generation (Example with Circular Fingerprints)\n",
    "\n",
    "### Important: Generate features AFTER splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26781707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circular_fingerprints(smiles_list, radius=2, n_bits=1024):\n",
    "    \"\"\"\n",
    "    Generate Morgan (circular) fingerprints.\n",
    "    \"\"\"\n",
    "    fps = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, n_bits)\n",
    "            fps.append(list(fp))\n",
    "        else:\n",
    "            fps.append([0] * n_bits)\n",
    "    return np.array(fps)\n",
    "\n",
    "# Generate fingerprints for each split\n",
    "print(\"Generating circular fingerprints...\")\n",
    "X_train = generate_circular_fingerprints(train_df['Canonical SMILES'])\n",
    "X_val = generate_circular_fingerprints(val_df['Canonical SMILES'])\n",
    "X_test = generate_circular_fingerprints(test_df['Canonical SMILES'])\n",
    "\n",
    "# Get targets\n",
    "y_train = train_df['IC50 uM'].values.astype(float)\n",
    "y_val = val_df['IC50 uM'].values.astype(float)\n",
    "y_test = test_df['IC50 uM'].values.astype(float)\n",
    "\n",
    "print(f\"‚úì Features generated:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   X_val: {X_val.shape}\")\n",
    "print(f\"   X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc0b5f",
   "metadata": {},
   "source": [
    "## Step 8: Target Transformation - BEFORE Scaling\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistake:\n",
    "```python\n",
    "# DON'T normalize target using full dataset statistics!\n",
    "df['IC50_norm'] = (df['IC50'] - df['IC50'].mean()) / df['IC50'].std()\n",
    "```\n",
    "\n",
    "### ‚úÖ Solution:\n",
    "- Transform target (e.g., pIC50, log) BEFORE splitting\n",
    "- OR fit scaler on train only, then transform val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to pIC50 (common in QSAR)\n",
    "def to_pIC50(ic50_uM):\n",
    "    \"\"\"Convert IC50 (ŒºM) to pIC50 = -log10(IC50 in M)\"\"\"\n",
    "    return -np.log10(ic50_uM * 1e-6)\n",
    "\n",
    "y_train_transformed = to_pIC50(y_train)\n",
    "y_val_transformed = to_pIC50(y_val)\n",
    "y_test_transformed = to_pIC50(y_test)\n",
    "\n",
    "print(\"‚úì Target transformed to pIC50\")\n",
    "print(f\"   Train: {y_train_transformed.shape}, range [{y_train_transformed.min():.2f}, {y_train_transformed.max():.2f}]\")\n",
    "print(f\"   Val: {y_val_transformed.shape}, range [{y_val_transformed.min():.2f}, {y_val_transformed.max():.2f}]\")\n",
    "print(f\"   Test: {y_test_transformed.shape}, range [{y_test_transformed.min():.2f}, {y_test_transformed.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087d4cc",
   "metadata": {},
   "source": [
    "## Step 9: Feature Scaling - FIT ON TRAIN ONLY! üö®\n",
    "\n",
    "### ‚ö†Ô∏è CRITICAL MISTAKE (causes leakage):\n",
    "```python\n",
    "# NEVER DO THIS!\n",
    "scaler = StandardScaler()\n",
    "X_all_scaled = scaler.fit_transform(X_all)  # Uses test statistics!\n",
    "```\n",
    "\n",
    "### ‚úÖ CORRECT WAY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# FIT scaler on TRAINING data ONLY\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# TRANSFORM (not fit_transform!) validation and test\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ CORRECT: Scaler fitted on training data only\")\n",
    "print(\"‚úÖ CORRECT: Validation and test transformed using training statistics\")\n",
    "print(f\"\\n   X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"   X_val_scaled: {X_val_scaled.shape}\")\n",
    "print(f\"   X_test_scaled: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9978042",
   "metadata": {},
   "source": [
    "## Step 10: Scaffold-Based Cross-Validation\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistake:\n",
    "```python\n",
    "# Random K-Fold - causes leakage!\n",
    "KFold(n_splits=5, shuffle=True)\n",
    "```\n",
    "\n",
    "### ‚úÖ Solution: Scaffold-based K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaffold-based K-fold splits\n",
    "cv_splits = splitter.scaffold_kfold(train_df, n_splits=5, random_state=42)\n",
    "\n",
    "print(f\"\\n‚úì Created {len(cv_splits)} scaffold-based CV folds\")\n",
    "print(\"\\nUse these splits for:\")\n",
    "print(\"  ‚Ä¢ Hyperparameter tuning\")\n",
    "print(\"  ‚Ä¢ Model selection\")\n",
    "print(\"  ‚Ä¢ Performance estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f44d2",
   "metadata": {},
   "source": [
    "## Step 11: Model Training Example (with Proper Pipeline)\n",
    "\n",
    "### Example: Random Forest with proper cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb95ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Train model on training data\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,  # Control complexity for small data\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training model on training set only...\")\n",
    "model.fit(X_train_scaled, y_train_transformed)\n",
    "\n",
    "# Evaluate on each split\n",
    "for split_name, X_split, y_split in [\n",
    "    ('Train', X_train_scaled, y_train_transformed),\n",
    "    ('Validation', X_val_scaled, y_val_transformed),\n",
    "    ('Test', X_test_scaled, y_test_transformed)\n",
    "]:\n",
    "    y_pred = model.predict(X_split)\n",
    "    rmse = np.sqrt(mean_squared_error(y_split, y_pred))\n",
    "    mae = mean_absolute_error(y_split, y_pred)\n",
    "    r2 = r2_score(y_split, y_pred)\n",
    "    \n",
    "    print(f\"\\n{split_name} Set Performance:\")\n",
    "    print(f\"   RMSE: {rmse:.3f}\")\n",
    "    print(f\"   MAE: {mae:.3f}\")\n",
    "    print(f\"   R¬≤: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ba37e",
   "metadata": {},
   "source": [
    "## Step 12: Applicability Domain Check\n",
    "\n",
    "### Report which test molecules are within the model's applicability domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0908c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check applicability domain for test set\n",
    "train_fps = [AllChem.GetMorganFingerprintAsBitVect(\n",
    "    Chem.MolFromSmiles(smi), 2, 2048) \n",
    "    for smi in train_df['Canonical SMILES']]\n",
    "\n",
    "test_ad_scores = []\n",
    "for smi in test_df['Canonical SMILES']:\n",
    "    test_fp = AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smi), 2, 2048)\n",
    "    ad_score = processor.estimate_applicability_domain(train_fps, test_fp, k=5)\n",
    "    test_ad_scores.append(ad_score)\n",
    "\n",
    "test_df['AD_Score'] = test_ad_scores\n",
    "\n",
    "# Molecules with AD score < 0.5 are outside applicability domain\n",
    "within_ad = test_df[test_df['AD_Score'] >= 0.5]\n",
    "outside_ad = test_df[test_df['AD_Score'] < 0.5]\n",
    "\n",
    "print(f\"\\nüìä Applicability Domain Analysis:\")\n",
    "print(f\"   Molecules within AD (score ‚â• 0.5): {len(within_ad)} ({len(within_ad)/len(test_df)*100:.1f}%)\")\n",
    "print(f\"   Molecules outside AD (score < 0.5): {len(outside_ad)} ({len(outside_ad)/len(test_df)*100:.1f}%)\")\n",
    "print(f\"\\n   Mean AD score: {np.mean(test_ad_scores):.3f}\")\n",
    "print(f\"   Median AD score: {np.median(test_ad_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a6179",
   "metadata": {},
   "source": [
    "## Summary: Data Leakage Checklist ‚úÖ\n",
    "\n",
    "### Before submitting your QSAR model, verify:\n",
    "\n",
    "- [ ] **Splitting Strategy**\n",
    "  - Used scaffold-based (not random) splitting\n",
    "  - Reported scaffold overlap = 0%\n",
    "  \n",
    "- [ ] **Duplicates**\n",
    "  - Canonicalized SMILES\n",
    "  - Removed exact duplicates before splitting\n",
    "  - Checked for near-duplicates (Tanimoto ‚â• 0.95)\n",
    "  \n",
    "- [ ] **Feature Engineering**\n",
    "  - Fitted scaler/normalizer on TRAIN only\n",
    "  - Applied transformation to val/test separately\n",
    "  - No feature selection on full dataset\n",
    "  \n",
    "- [ ] **Target Variable**\n",
    "  - Transformed before splitting OR fitted on train only\n",
    "  - No target-derived features\n",
    "  \n",
    "- [ ] **Cross-Validation**\n",
    "  - Used scaffold-based K-fold (not random)\n",
    "  - Nested CV for hyperparameter tuning\n",
    "  \n",
    "- [ ] **Reporting**\n",
    "  - Train-test similarity distribution shown\n",
    "  - Applicability domain reported\n",
    "  - Model complexity appropriate for data size\n",
    "  \n",
    "- [ ] **External Validation** (if available)\n",
    "  - Completely unseen compounds\n",
    "  - No SMILES overlap with training\n",
    "  - Performance on external set reported\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Next Steps:\n",
    "1. Apply this approach to Model 1, 2, 3, and 4\n",
    "2. Re-train models with proper splitting\n",
    "3. Compare performance (expect lower but more realistic metrics)\n",
    "4. Document all leakage prevention steps\n",
    "5. Prepare for publication/review with proper methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bbc48",
   "metadata": {},
   "source": [
    "## Save Cleaned Data for Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c82d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the splits for use in other models\n",
    "train_df.to_csv('train_set_no_leakage.csv', index=False)\n",
    "val_df.to_csv('val_set_no_leakage.csv', index=False)\n",
    "test_df.to_csv('test_set_no_leakage.csv', index=False)\n",
    "\n",
    "# Save indices for reproducibility\n",
    "np.save('train_indices.npy', train_idx)\n",
    "np.save('val_indices.npy', val_idx)\n",
    "np.save('test_indices.npy', test_idx)\n",
    "\n",
    "print(\"‚úì Data saved successfully!\")\n",
    "print(\"\\nUse these files in Models 1, 2, 3, and 4 to ensure consistency.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
