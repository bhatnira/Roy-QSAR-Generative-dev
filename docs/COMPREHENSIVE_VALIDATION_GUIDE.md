# üéØ Comprehensive QSAR Model Validation Guide

## Critical Issues Addressed (Beyond Data Leakage)

This guide extends the data leakage prevention measures with **13 additional critical validation steps** for low-data QSAR models.

---

## üìã Table of Contents

1. [Dataset Bias & Representativeness](#1-dataset-bias--representativeness)
2. [Overfitting Due to Model Complexity](#2-overfitting-due-to-model-complexity)
3. [Improper Cross-Validation Design](#3-improper-cross-validation-design)
4. [Measurement Noise & Assay Variability](#4-measurement-noise--assay-variability)
5. [Activity Cliffs](#5-activity-cliffs)
6. [Target Imbalance & Range Compression](#6-target-imbalance--range-compression)
7. [Descriptor & Fingerprint Limitations](#7-descriptor--fingerprint-limitations)
8. [Poor Uncertainty Estimation](#8-poor-uncertainty-estimation)
9. [Improper Performance Metrics](#9-improper-performance-metrics)
10. [Lack of Baseline & Negative Controls](#10-lack-of-baseline--negative-controls)
11. [Interpretability Overclaiming](#11-interpretability-overclaiming)
12. [Reproducibility & Transparency](#12-reproducibility--transparency)
13. [External Validity Overstatement](#13-external-validity-overstatement)

---

## 1. Dataset Bias & Representativeness

### ‚ö†Ô∏è Issues
- **Chemical space is narrow or congeneric**: Dataset dominated by a single scaffold series
- **Overrepresentation of potent or inactive compounds**: Biased toward "successful" compounds
- **Public datasets biased**: Often from focused screening campaigns

### üî¥ Consequences
- Artificially high R¬≤ / low RMSE
- Poor generalization to novel scaffolds
- False sense of model quality

### ‚úÖ Mitigation

```python
from qsar_validation_utils import DatasetBiasAnalyzer

# Analyze scaffold diversity
analyzer = DatasetBiasAnalyzer(smiles_col='Canonical SMILES', target_col='IC50 uM')
diversity_results = analyzer.analyze_scaffold_diversity(df)

# Key metrics to report:
# - Diversity ratio (unique scaffolds / total molecules)
# - Gini coefficient (scaffold distribution inequality)
# - Top scaffold representation
```

**What to report in paper:**
- Number of unique scaffolds
- Diversity ratio
- Top 5 scaffolds and their representation
- Statement: "Dataset contains X unique scaffolds (Y% diversity). Top scaffold represents Z% of molecules, indicating [congeneric/diverse] chemical space."

**Red flags:**
- Diversity ratio < 0.3 ‚Üí Congeneric series
- Top scaffold > 50% ‚Üí Dominated by one series
- Gini coefficient > 0.6 ‚Üí High imbalance

---

## 2. Overfitting Due to Model Complexity

### ‚ö†Ô∏è Issues
- **Too many parameters vs samples**: Deep learning with n < 500
- **Excessive hyperparameter tuning**: Testing 100s of configurations
- **No regularization**: Unregularized complex models

### üî¥ Consequences
- Memorization instead of learning
- High CV performance, poor external test results
- Non-reproducible results

### ‚úÖ Mitigation

```python
from qsar_validation_utils import ModelComplexityAnalyzer

# Analyze complexity
ModelComplexityAnalyzer.analyze_complexity(
    n_samples=len(df),
    n_features=X.shape[1],
    model_type='random_forest'  # or 'deep_learning', 'gaussian_process', etc.
)
```

**Guidelines:**

| Samples:Features Ratio | Risk Level | Recommendation |
|------------------------|-----------|----------------|
| < 5:1 | üî¥ Critical | Strong regularization REQUIRED |
| 5-10:1 | üü† High | Use regularized models only |
| 10-20:1 | üü° Moderate | Standard regularization |
| > 20:1 | ‚úì Good | Can use complex models carefully |

**Model-specific advice:**

- **Deep Learning**: Need n > 1000 OR use pre-trained embeddings (ChEBERTa)
- **Random Forest**: Limit `max_depth=3-5`, increase `min_samples_leaf=5-10`
- **Gradient Boosting**: Low `learning_rate=0.01-0.05`, limit `max_depth=2-4`
- **Gaussian Process**: Works well for n < 500, provides uncertainty
- **Linear (Ridge/Lasso)**: Works well even with n < 100

üìå **In low-data QSAR, model simplicity often wins.**

---

## 3. Improper Cross-Validation Design

### ‚ö†Ô∏è Issues
- **Random CV instead of scaffold CV**: Leaks information through similar molecules
- **Using CV folds for feature selection**: Optimistic bias
- **Reporting best fold instead of average**: Cherry-picking

### üî¥ Consequences
- Optimistic performance estimates
- Non-reproducible results
- Reviewer criticism

### ‚úÖ Mitigation

```python
from qsar_utils_no_leakage import ScaffoldSplitter

splitter = ScaffoldSplitter(smiles_col='Canonical SMILES')

# For hyperparameter tuning: scaffold-based K-fold
cv_splits = splitter.scaffold_kfold(df, n_splits=5, random_state=42)

# For each fold:
for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
    # 1. Extract training and validation data
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    # 2. Fit scaler on TRAINING ONLY
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # 3. Train model on training fold
    model.fit(X_train_scaled, y_train)
    
    # 4. Evaluate on validation fold
    y_pred = model.predict(X_val_scaled)
    # ... calculate metrics ...
```

**What to report:**
- Mean ¬± std across ALL folds
- Not just best fold
- Use scaffold-based CV, not random

---

## 4. Measurement Noise & Assay Variability

### ‚ö†Ô∏è Issues
- **Mixed assay types**: IC‚ÇÖ‚ÇÄ, EC‚ÇÖ‚ÇÄ, K·µ¢ from different protocols
- **Different protocols**: pH, temperature, cell lines, species
- **Large experimental error**: Often ¬±0.3‚Äì0.6 log units for IC‚ÇÖ‚ÇÄ

### üî¥ Consequences
- **Upper bound on achievable performance**: Cannot predict better than assay precision
- **Models learn assay artifacts**: Different IC‚ÇÖ‚ÇÄ values for same compound
- **Suspicious low RMSE**: RMSE < 0.3 log units is usually overfitting

### ‚úÖ Mitigation

```python
from qsar_validation_utils import AssayNoiseEstimator

# Estimate experimental error
noise_estimator = AssayNoiseEstimator()
error_estimate = noise_estimator.estimate_experimental_error(
    df, 
    target_col='IC50 uM'
)

# Compare model error to assay error
print(f"Model RMSE: {model_rmse:.3f}")
print(f"Expected assay error: {error_estimate['experimental_error']:.3f}")

if model_rmse < error_estimate['experimental_error']:
    print("‚ö†Ô∏è Model RMSE lower than assay precision - possible overfitting")
```

**Typical IC‚ÇÖ‚ÇÄ/EC‚ÇÖ‚ÇÄ experimental error: 0.3 ‚Äì 0.6 log units**

**What to report:**
- "Considering typical IC‚ÇÖ‚ÇÄ assay reproducibility (¬±0.5 log units), our model achieves near-optimal predictive performance."
- Always report RMSE in context of assay precision

üìå **RMSE < assay error is usually suspicious.**

---

## 5. Activity Cliffs

### ‚ö†Ô∏è Issues
- **Small structural changes ‚Üí large activity changes**: SAR discontinuities
- **Fingerprints treat similar molecules as "close"**: But activity differs dramatically
- **Major QSAR limitation**: Affects all similarity-based methods

### üî¥ Consequences
- Poor local predictivity
- Unstable SAR interpretation
- Feature importance misleading

### ‚úÖ Mitigation

```python
from qsar_validation_utils import ActivityCliffDetector

detector = ActivityCliffDetector(smiles_col='Canonical SMILES', target_col='IC50 uM')

# Detect activity cliffs
cliff_df = detector.detect_activity_cliffs(
    df,
    similarity_threshold=0.85,  # Tanimoto similarity
    activity_threshold=2.0       # 100-fold difference (2 log units)
)

print(f"Found {len(cliff_df)} activity cliff pairs")
```

**If activity cliffs detected:**
- Consider local models (separate models per scaffold)
- Use Gaussian Processes (handle discontinuities better)
- Do NOT overinterpret feature importance
- State in paper: "X activity cliffs detected, limiting local predictivity"

---

## 6. Target Imbalance & Range Compression

### ‚ö†Ô∏è Issues
- **Activity values clustered in narrow range**: Limited variance to predict
- **Few high- or low-potency compounds**: Imbalanced distribution
- **Artificial truncation**: Filtering out inactives

### üî¥ Consequences
- **Inflated R¬≤**: High correlation even with poor predictions
- **Poor extrapolation**: Model hasn't seen diverse activities
- **Misleading performance**: R¬≤ looks great, but predictions unreliable

### ‚úÖ Mitigation

```python
from qsar_validation_utils import DatasetBiasAnalyzer

analyzer = DatasetBiasAnalyzer(smiles_col='Canonical SMILES', target_col='IC50 uM')
activity_stats = analyzer.analyze_activity_distribution(df)

# Check relative range
relative_range = (activity_stats['max'] - activity_stats['min']) / activity_stats['mean']

if relative_range < 2.0:
    print("‚ö†Ô∏è Narrow activity range - R¬≤ may be inflated")
    print("   Focus on RMSE/MAE for evaluation")
```

**What to report:**
- Activity range: [min, max]
- Mean ¬± std
- Distribution plot (histogram)
- Statement about range limitations

**Use RMSE/MAE in addition to R¬≤ - don't rely on R¬≤ alone!**

---

## 7. Descriptor & Fingerprint Limitations

### ‚ö†Ô∏è Issues
- **High dimensionality vs small N**: 1024 fingerprint bits with n < 200
- **Collinear descriptors**: Many RDKit descriptors correlated
- **Binary fingerprints losing meaning**: 0/1 loses physicochemical information

### üî¥ Consequences
- Unstable models
- Non-interpretable results
- Overfitting

### ‚úÖ Mitigation

```python
# 1. Use regularization (always!)
from sklearn.linear_model import Ridge, Lasso, ElasticNet

model = Ridge(alpha=1.0)  # L2 regularization

# 2. For RDKit descriptors, remove highly correlated
correlation_matrix = X_train.corr()
# Remove features with |correlation| > 0.95

# 3. For fingerprints, consider lower dimensions
# 512 or 256 bits instead of 2048

# 4. Use PCA or feature selection (on TRAINING only!)
from sklearn.decomposition import PCA

pca = PCA(n_components=50)
X_train_reduced = pca.fit_transform(X_train)
X_test_reduced = pca.transform(X_test)  # Apply to test
```

---

## 8. Poor Uncertainty Estimation

### ‚ö†Ô∏è Issues
- **Point predictions only**: No confidence intervals
- **No confidence assessment**: Don't know when model is uncertain
- **Overconfident predictions outside domain**: Extrapolation without warning

### üî¥ Consequences
- Unsafe decision making
- Wasted experimental effort on uncertain predictions
- Reviewer criticism

### ‚úÖ Mitigation

**Option 1: Gaussian Process Regression**
```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

kernel = RBF() + WhiteKernel()
gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-5)
gpr.fit(X_train, y_train)

# Get predictions with uncertainty
y_pred, y_std = gpr.predict(X_test, return_std=True)

# 95% confidence interval
lower_bound = y_pred - 1.96 * y_std
upper_bound = y_pred + 1.96 * y_std
```

**Option 2: Ensemble Methods**
```python
from sklearn.ensemble import RandomForestRegressor

# Random Forest provides prediction variance
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train, y_train)

# Get predictions from all trees
predictions = np.array([tree.predict(X_test) for tree in rf.estimators_])
y_pred = np.mean(predictions, axis=0)
y_std = np.std(predictions, axis=0)
```

**Option 3: Applicability Domain Check**
```python
from qsar_utils_no_leakage import QSARDataProcessor

processor = QSARDataProcessor()
similarity_stats = processor.analyze_similarity(df, train_idx, test_idx)

# Flag low-confidence predictions
for i, sim in enumerate(similarity_stats['similarities']):
    if sim < 0.5:  # Low similarity to training
        print(f"‚ö†Ô∏è Test molecule {i}: Low confidence (max similarity={sim:.2f})")
```

---

## 9. Improper Performance Metrics

### ‚ö†Ô∏è Issues
- **Reporting R¬≤ only**: Misleading with narrow activity ranges
- **No external test metrics**: Only CV reported
- **No baseline comparison**: Can't judge if model is learning

### üî¥ Consequences
- Misleading performance claims
- Can't assess real predictive power
- Difficult to compare across studies

### ‚úÖ Mitigation

```python
from qsar_validation_utils import PerformanceMetricsCalculator

calculator = PerformanceMetricsCalculator()

# Calculate ALL metrics
metrics = calculator.calculate_all_metrics(y_true, y_pred, set_name="Test")

# Metrics calculated:
# - RMSE (primary metric for QSAR)
# - MAE (robust to outliers)
# - R¬≤ (correlation, but can be misleading)
# - Spearman œÅ (rank correlation, robust)
# - Pearson r (linear correlation)
```

**What to report in paper:**

| Metric | Train | Validation | Test | External Test |
|--------|-------|------------|------|---------------|
| RMSE | X.XX | X.XX ¬± X.XX | X.XX | X.XX |
| MAE | X.XX | X.XX ¬± X.XX | X.XX | X.XX |
| R¬≤ | X.XX | X.XX ¬± X.XX | X.XX | X.XX |
| Spearman œÅ | X.XX | X.XX ¬± X.XX | X.XX | X.XX |

**Always report:**
1. RMSE (most important for QSAR)
2. MAE (interpretable, robust)
3. R¬≤ (with caution if narrow range)
4. Spearman œÅ (robust to outliers)

---

## 10. Lack of Baseline & Negative Controls

### ‚ö†Ô∏è Issues
- **No comparison to simple models**: Is complex model worth it?
- **No randomization tests**: Is model learning real signal?
- **No null hypothesis testing**: Chance correlation?

### üî¥ Consequences
- Can't judge if complex model justified
- May be learning noise
- Overfitting undetected

### ‚úÖ Mitigation

**A. Baseline Model (Ridge Regression)**

```python
from qsar_validation_utils import PerformanceMetricsCalculator

# Calculate baseline performance
baseline_metrics = PerformanceMetricsCalculator.calculate_baseline_metrics(
    X, y, cv_folds=cv_splits
)

print(f"Baseline Ridge: RMSE = {baseline_metrics['rmse_mean']:.3f}")
print(f"Your model:     RMSE = {your_model_rmse:.3f}")

if your_model_rmse > baseline_metrics['rmse_mean']:
    print("‚ö†Ô∏è Complex model WORSE than simple baseline - overfitting likely")
```

**B. Y-Randomization Test (Y-Scrambling)**

```python
from qsar_validation_utils import YRandomizationTester

# Test if model learns real signal or noise
tester = YRandomizationTester()
randomization_results = tester.perform_y_randomization(
    X, y, model, n_iterations=10, cv_folds=cv_splits
)

print(f"Y-scrambled R¬≤: {randomization_results['r2_mean']:.3f}")
print(f"Real data R¬≤:   {real_r2:.3f}")

# R¬≤ with scrambled data should be ‚â§ 0
# If R¬≤ > 0.2 with scrambled ‚Üí overfitting!
```

**What to report:**
- "Simple Ridge regression baseline: RMSE = X.XX"
- "Our model: RMSE = X.XX (Y% improvement)"
- "Y-randomization test: R¬≤ = X.XX ¬± X.XX (should be ‚â§ 0)"

üìå **If model beats y-randomization only marginally ‚Üí weak signal.**

---

## 11. Interpretability Overclaiming

### ‚ö†Ô∏è Issues
- **Overinterpreting SHAP on correlated features**: Attributions unstable
- **Claiming mechanistic insights from fingerprints**: Bit 347 doesn't have chemical meaning
- **Feature importance = causation**: Correlation ‚â† causation

### üî¥ Consequences
- Scientifically misleading conclusions
- False mechanistic claims
- Wastes experimental validation effort

### ‚úÖ Mitigation

**Safe interpretation practices:**

```python
# SHAP for feature importance (use with caution)
import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Plot
shap.summary_plot(shap_values, X_test)
```

**How to report:**

‚úÖ **CORRECT:**
- "Feature X shows high importance, suggesting potential role (hypothesis-generating)"
- "Results consistent with known SAR for this target"
- "Interpretation should be validated experimentally"

‚ùå **INCORRECT:**
- "Feature X causes activity"
- "This proves the mechanism is..."
- "Bit 347 indicates specific substructure importance"

**Guidelines:**
1. Treat feature importance as **hypothesis-generating only**
2. Validate against known SAR
3. Don't claim mechanistic insights from fingerprint bits
4. State limitations clearly
5. For mechanistic claims, need experimental validation

---

## 12. Reproducibility & Transparency

### ‚ö†Ô∏è Issues
- **No code shared**: Can't reproduce results
- **No split seed reported**: Different splits = different results
- **No dataset versioning**: Data changed over time
- **No software versions**: Package updates break reproduction

### üî¥ Consequences
- Rejection or major revision
- Can't build on your work
- Scientific credibility damaged

### ‚úÖ Mitigation

**Required for reproducibility:**

```python
# 1. Set ALL random seeds
import random
import numpy as np

RANDOM_SEED = 42

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
# Also set seeds for: tensorflow, torch, etc.

# 2. Save data split indices
np.savez('data_splits.npz', 
         train_idx=train_idx,
         val_idx=val_idx, 
         test_idx=test_idx,
         random_seed=RANDOM_SEED)

# 3. Record software versions
import sys
import sklearn
import rdkit

print(f"Python: {sys.version}")
print(f"scikit-learn: {sklearn.__version__}")
print(f"RDKit: {rdkit.__version__}")

# 4. Save preprocessing parameters
preprocessing_params = {
    'scaler_mean': scaler.mean_.tolist(),
    'scaler_std': scaler.scale_.tolist(),
    'features_used': feature_names,
    'preprocessing_steps': ['canonicalize', 'deduplicate', 'scaffold_split']
}
import json
with open('preprocessing_params.json', 'w') as f:
    json.dump(preprocessing_params, f)
```

**What to share publicly:**

1. ‚úÖ **Code**: GitHub/GitLab repository
2. ‚úÖ **Data**: Zenodo DOI (or supplementary)
3. ‚úÖ **Split indices**: Exact train/val/test splits
4. ‚úÖ **Preprocessing parameters**: Scaler, feature selection, etc.
5. ‚úÖ **Model weights**: Trained model file
6. ‚úÖ **Requirements**: `requirements.txt` or `environment.yml`

**In paper supplementary:**
- Full preprocessing steps
- Hyperparameter search space
- Software versions
- Random seeds
- Link to code/data repository

---

## 13. External Validity Overstatement

### ‚ö†Ô∏è Issues
- **Claiming "general QSAR model"**: Tested on one target only
- **Extrapolating beyond chemical space**: Predictions outside training domain
- **Overstating applicability**: "Works for all kinase inhibitors"

### üî¥ Consequences
- False expectations
- Wasted experimental effort
- Damaged reputation

### ‚úÖ Mitigation

**Define applicability domain:**

```python
from qsar_utils_no_leakage import QSARDataProcessor

processor = QSARDataProcessor()

# For each new prediction:
def predict_with_domain_check(model, X_new, X_train_fingerprints, threshold=0.5):
    """
    Predict with applicability domain check.
    """
    # Generate fingerprint for new molecule
    fp_new = # ... generate fingerprint ...
    
    # Check similarity to training set
    similarities = [DataStructs.TanimotoSimilarity(fp_new, fp_train) 
                   for fp_train in X_train_fingerprints]
    max_sim = max(similarities)
    
    # Make prediction
    prediction = model.predict(X_new)
    
    # Flag if outside domain
    if max_sim < threshold:
        confidence = "LOW"
        warning = f"‚ö†Ô∏è Outside applicability domain (max sim={max_sim:.2f})"
    else:
        confidence = "HIGH"
        warning = None
    
    return prediction, confidence, warning
```

**How to state applicability:**

‚úÖ **CORRECT:**
- "Model applicable to [target] inhibitors with [scaffold types]"
- "Trained on chemical space with Tanimoto similarity ‚â• 0.5 to [reference set]"
- "Applicability domain: [chemical descriptors/scaffolds]"
- "External validation on [specific test set]: RMSE = X.XX"

‚ùå **INCORRECT:**
- "General QSAR model for all drug-like molecules"
- "Applicable to any kinase inhibitor"
- "Works for lead optimization of any scaffold"

**In paper:**
1. Clearly state target(s) covered
2. Define chemical space (scaffolds, descriptors)
3. State applicability domain explicitly
4. Note limitations
5. Suggest when predictions reliable vs uncertain

---

## üìä Summary: Issue Severity

| Category | Severity | Impact | Fix Difficulty |
|----------|----------|--------|----------------|
| Data leakage | üî¥ Critical | Invalidates results | Medium |
| Dataset bias | üî¥ Critical | Limits generalization | Low (report) |
| Model overfitting | üî¥ Critical | Non-reproducible | Medium |
| Improper CV | üî¥ Critical | Optimistic metrics | Medium |
| Assay noise | üü† High | Unrealistic expectations | Low (report) |
| Activity cliffs | üü† High | Local unpredictability | Low (detect) |
| Metric misuse | üü† High | Misleading conclusions | Low (add metrics) |
| No baseline | üü† High | Can't judge quality | Low (add baseline) |
| No y-randomization | üü† High | Overfitting undetected | Low (add test) |
| Poor uncertainty | üü° Moderate | Unsafe predictions | Medium (add GPR) |
| Interpretability | üü° Moderate | Scientific misuse | Low (state limits) |
| Reproducibility | üü° Moderate | Can't reproduce | Low (share code) |
| Validity overstatement | üü° Moderate | False expectations | Low (state limits) |

---

## üéØ Quick Implementation Checklist

### Before Training Model:

- [ ] Scaffold diversity analysis
- [ ] Activity distribution analysis
- [ ] Activity cliff detection
- [ ] Check samples:features ratio
- [ ] Estimate experimental error

### During Model Development:

- [ ] Use scaffold-based splitting
- [ ] Use scaffold-based cross-validation
- [ ] Apply appropriate regularization
- [ ] Fit scaler on training only
- [ ] Set random seeds

### After Training Model:

- [ ] Calculate all metrics (RMSE, MAE, R¬≤, Spearman)
- [ ] Run baseline model (Ridge)
- [ ] Perform y-randomization test
- [ ] Check predictions vs assay precision
- [ ] Define applicability domain
- [ ] Report scaffold diversity per split

### For Paper:

- [ ] Report all metrics (not just R¬≤)
- [ ] State experimental error context
- [ ] Report y-randomization results
- [ ] Show baseline comparison
- [ ] State applicability domain
- [ ] Share code/data/splits
- [ ] Acknowledge limitations

---

## üìö Expected Performance After Fixes

When implementing all these fixes, **expect performance to drop**:

| Metric | Before Fixes | After Fixes | Interpretation |
|--------|--------------|-------------|----------------|
| R¬≤ | 0.85 | 0.60 | ‚úì More realistic |
| RMSE | 0.25 | 0.50 | ‚úì Near assay limit |
| Test performance | High | Lower | ‚úì Tests generalization |

**This is NORMAL and CORRECT!**

- If performance stays very high ‚Üí still may have issues
- RMSE ‚âà 0.5 log units is near theoretical limit
- Scaffold split should be harder than random split
- Lower performance with proper validation = honest results

---

## üéì Key Takeaways

1. **Simplicity wins in low-data regime** (n < 200)
2. **Scaffold split mandatory** for honest evaluation  
3. **RMSE ‚âà 0.5 log units** is near theoretical limit for IC‚ÇÖ‚ÇÄ
4. **R¬≤ alone misleading** with narrow activity ranges
5. **Y-randomization catches overfitting**
6. **Activity cliffs limit predictivity**
7. **Report baseline comparison** (Ridge regression)
8. **State limitations honestly** ‚Üí better reviews
9. **Share code/data/splits** for reproducibility
10. **Don't overclaim** interpretability or applicability

---

## üìñ Additional Resources

- [QSAR Model Reporting Guidelines (OECD)](https://www.oecd.org/chemicalsafety/risk-assessment/qsar-toolbox.htm)
- [Best Practices in QSAR Modeling (J. Chem. Inf. Model.)](https://pubs.acs.org/doi/10.1021/ci800309x)
- [Activity Cliffs in Drug Discovery (J. Med. Chem.)](https://pubs.acs.org/doi/10.1021/jm901739z)
- [Y-Randomization in QSAR (QSAR Comb. Sci.)](https://onlinelibrary.wiley.com/doi/10.1002/qsar.200710007)

---

**Generated for comprehensive QSAR validation**  
**January 2026**
